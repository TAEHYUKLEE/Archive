<!DOCTYPE html>
<html lang="en">
    
<head>
    <title>TAYLEE Research</title>

    <!--Style css Link-->
    <link rel="stylesheet" type="text/css" href="../package/css/style research.css">
    <link rel="stylesheet" type="text/css" href="../package/css/style.css">

    <!--Java script event-->
    <script src="../package/js/main.js" defer></script>
    <script src="../package/js/components/nav.js" sync></script>
    <script src="../package/js/components/footer.js" sync></script>
    <script src="../package/js/components/header.js" sync></script>

    <common-header-component></common-header-component>
    
</head>



<body bgproperties="fixed" oncontextmenu='return false'>

    <other-nav-component></other-nav-component>
    
    <div class = "frame"> <!--div for grouping-->
    
        <!--CFD Control-->
        <h3 class="RL">Deep Reinforcement learning (applied to CFD)</h3>
        <h4 class="DRL-title">TD3 (off-policy) & PPO (on-policy) - Validation for DRL algorithm</h4>  
        
        <div class="Burgers">

            <div class="Control-div">
                <img id="burgers", src="../resources/reinforcement/Burgers equation nu=0.1.gif" width=360 height=330>
                <p class="burgers_cap">(a) burgers equation (no control)</p>
            </div>

            <div class="Control-div">
                <img id="burgers", src="../resources/reinforcement/TD3 burgers.gif" width=360 height=330>
                <p class="TD3_cap">(b) TD3-Control</p>
            </div>

            <div class="Control-div">
                <img id="burgers", src="../resources/reinforcement/ppo burgers.gif" width=360 height=330>
                <p class="PPO_cap">(c) PPO-Control</p>
            </div>    

        </div>        
        
        <p class="burgers-description">&nbsp <b>Description</b>: Machine learning is applied with CFD as a domain. Here, the Burgers equation is simply Controlled by reinforcement learning is shown above.
            (Burgers equation is one-dimensional Navier-Stoeks equation - here it is solved by spectral method). 
            (a) is the burgers equation without control. Initial condition is sinusoidal, you can see that the sin waveform is broken down by the viscous term.
            The results of training with reinforcement learning to maintain the initial state (sin waveform) are (b) and (c).
            (b) is implemented with the TD3 algorithm of the deterministic policy family,
            (c) is implemented as a PPO algorithm of the stochastic policy family.
            <br></p>
    
        <!--CFD Control-->
        <h4 class="DRL-title">Turbulence control for drag reduction through deep reinforcement learning</h4>  
        
        <div class="Turb_con">

            <div class="Control-div">
                <img id="Turb_con_CNN", src="../resources/reinforcement/CNN - Network.jpg" width=700 height=470>
                <p class="Turb_con_cap">(a) Actor-Critic network structure</p>
            </div>

        </div>   
        
        <div class="Turb_con">

            <div class="Control-div">
                <img id="Turb_con_shear", src="../resources/reinforcement/no control 360.gif" width=520 height=470>
                <p class="Turb_con_cap">(b) No control shear field</p>
            </div>

            <div class="Control-div">
                <img id="Turb_con_shear", src="../resources/reinforcement/Controlling shear flow.gif" width=520 height=470>
                <p class="Turb_con_cap">(c) Control of shear field</p>
            </div>

        </div>
        
        <p class="Turb_con-description">&nbsp <b>Description</b>: Drag reduction in a fluid is associated with a variety of social problems ranging from economic loss to environmental problems. The goal of this research is to develop a strategy
            for drag reduction in turbulent channel flow by controlling blowing suction using Deep reinforcement learning. As a result, performance is increased by approximately 5% over previous deep learning models. My involvement in this work was to construct a reinforcement
            learning algorithm directly in Tensorflow, connect it to CFD, and then trained the network
            and analyzed it. (a) Actor-Critic network structure inserted in TD3 algorithm, (b) No control shear field (c) As can be seen, the DRL control reduces the high shear stress (RED to BLUE).
            and  This is currently being worked on in preparation for submission to a journal thesis.
            <br><br></p> 
        
        <p class="Link_tur"> <a class = "conf_tur" href="https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE10607412">It is presented at the KSME 2021 spring Conference.</a> </p>
        <p class="Link_tur"> <a class = "conf_tur" href="https://journals.aps.org/prfluids/abstract/10.1103/PhysRevFluids.8.024604">publised in PRF(Physical Review Fluids)on 2023-02-08.</a> </p>

        <p class="bottom"></p>
    
    </div>

    <other-footer-component></other-footer-component>


</body>



</html>